name: Pegos Live Data Fetch & Predict

on:
  schedule:
    - cron: "0 6,10,14,18 * * *"   # UTC -> TR: 09:00, 13:00, 17:00, 21:00
  workflow_dispatch:

jobs:
  fetch_and_upload:
    runs-on: ubuntu-latest
    timeout-minutes: 50

    env:
      AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
      CT0: ${{ secrets.CT0 }}
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
      TWITTER_USER: ${{ secrets.TWITTER_USER }}
      TWITTER_PASS: ${{ secrets.TWITTER_PASS }}
      KEEP_LAST: ${{ secrets.KEEP_LAST }}

    steps:
      # ===========================
      # 0Ô∏è‚É£ Repo ve ortam kurulumu
      # ===========================
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Chrome & dependencies
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install Python dependencies
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===========================
      # DEBUG: Ortam & HF Token kontrol
      # ===========================
      - name: Debug environment & HF token
        run: |
          set -e
          echo "=== DEBUG ENV ==="
          echo "HF_DATASET_REPO=${HF_DATASET_REPO}"
          echo "AUTH_TOKEN length: ${#AUTH_TOKEN}"
          echo "CT0 length: ${#CT0}"
          echo "KEEP_LAST=${KEEP_LAST}"
          echo "HF_TOKEN length: ${#HF_TOKEN}"
          echo "huggingface_hub version:"
          python -m pip show huggingface_hub || true
          echo "HF whoami check:"
          python -c "import os; from huggingface_hub import HfApi; api=HfApi(token=os.getenv('HF_TOKEN')); print('OK:', bool(api.whoami()))"
          echo "=================="

      # ===========================
      # 1Ô∏è‚É£ Tweet scraping (Notebook)
      # ===========================
      - name: Run scraper notebook (data.ipynb)
        env:
          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
          CT0: ${{ secrets.CT0 }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
          TWITTER_USER: ${{ secrets.TWITTER_USER }}
          TWITTER_PASS: ${{ secrets.TWITTER_PASS }}
          KEEP_LAST: ${{ secrets.KEEP_LAST }}
        run: |
          set -e
          echo "üöÄ Starting Pegos Twitter Scraper..."
          jupyter nbconvert --to notebook --execute data.ipynb --output output.ipynb --ExecutePreprocessor.timeout=1800
          echo "‚úÖ Notebook executed successfully"

      # ===========================
      # 2Ô∏è‚É£ Hugging Face'e tweet upload (daily folder)
      # ===========================
      - name: Upload scraped data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          TODAY=$(date +'%Y-%m-%d')
          DAILY_CSV="/tmp/data/${TODAY}/pegos_output.csv"
          echo "üìÖ Checking for daily CSV at: $DAILY_CSV"
          if [ -f "$DAILY_CSV" ]; then
            echo "üì§ Uploading CSV for $TODAY..."
            export LOCAL_CSV="$DAILY_CSV"
            python upload_to_hf.py
          elif [ -f "/tmp/pegos_output.csv" ]; then
            echo "‚ö†Ô∏è Fallback: Uploading main /tmp/pegos_output.csv instead..."
            export LOCAL_CSV="/tmp/pegos_output.csv"
            python upload_to_hf.py
          else
            echo "‚ùå No CSV file found ‚Äî skipping upload."
          fi

      # ===========================
      # 3Ô∏è‚É£ Tweet + BTC verisi birle≈ütirme
      # ===========================
      - name: Merge and upload full dataset (tweets + BTC)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "üîÑ Building merged dataset..."
          python build_dataset_hf.py
          echo "‚úÖ Merged dataset uploaded to HF."

      # ===========================
      # 4Ô∏è‚É£ Temizleme (duplicate / outlier)
      # ===========================
      - name: Clean merged dataset (remove duplicates/outliers)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "üßπ Cleaning merged dataset..."
          python clean_dataset_hf.py
          echo "‚úÖ Cleaned dataset uploaded to HF."

      # ===========================
      # 5Ô∏è‚É£ Model Prediction (Hybrid: Direction + Price)
      # ===========================
      - name: Run Pegos Hybrid Prediction
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "ü§ñ Running Pegos hybrid prediction pipeline..."
          python predict_with_model.py
          echo "‚úÖ Hybrid model predictions completed and uploaded to HF (pred_label, pred_diff, predicted_price)."

      # ===========================
      # 6Ô∏è‚É£ HF dataset i√ßeriƒüini listele
      # ===========================
      - name: List HF dataset files
        if: always()
        run: |
          echo "üìÇ Listing HF dataset files..."
          python -c "import os; from huggingface_hub import HfApi; api=HfApi(token=os.getenv('HF_TOKEN')); repo=os.getenv('HF_DATASET_REPO'); [print(' -', f) for f in api.list_repo_files(repo_id=repo, repo_type='dataset')]"

      # ===========================
      # 7Ô∏è‚É£ Workspace dosyalarƒ±
      # ===========================
      - name: Show workspace files
        if: always()
        run: |
          echo 'üìÅ Workspace contents:'
          ls -lah /tmp || true
          if [ -d "/tmp/data" ]; then
            echo 'üìÖ Daily data folders:'
            ls -lah /tmp/data
          fi

      # ===========================
      # 8Ô∏è‚É£ Notebook √ßƒ±ktƒ± √∂zeti
      # ===========================
      - name: Show notebook output summary
        if: always()
        run: |
          echo "üìú Showing notebook text output (first 200 lines):"
          head -n 200 output.ipynb | sed 's/\\n/\n/g' | grep -E "‚úÖ|‚ö†Ô∏è|‚ùå|tweet|Upload|Total" || true
