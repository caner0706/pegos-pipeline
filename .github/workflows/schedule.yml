name: Pegos Live Data Fetch & Upload

on:
  schedule:
    - cron: "0 6,10,14,18 * * *"  # UTC saatine g√∂re (TR: 09:00, 13:00, 17:00, 21:00)
  workflow_dispatch:              # Manuel tetikleme i√ßin

jobs:
  fetch_and_upload:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
      TWITTER_USER: ${{ secrets.TWITTER_USER }}
      TWITTER_PASS: ${{ secrets.TWITTER_PASS }}
      KEEP_LAST: ${{ secrets.KEEP_LAST }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
          pip install selenium webdriver-manager beautifulsoup4 pandas huggingface_hub jupyter

      - name: Run scraper notebook (data.ipynb)
        run: |
          echo "üöÄ Starting Pegos Data Pipeline..."
          jupyter nbconvert --to notebook --execute data.ipynb --output output.ipynb --ExecutePreprocessor.timeout=1800
          echo "‚úÖ Notebook executed successfully"

      - name: Show result summary
        if: always()
        run: |
          echo "üìò Notebook execution finished."
          echo "üîç Checking last lines of output..."
          tail -n 30 output.ipynb || true
