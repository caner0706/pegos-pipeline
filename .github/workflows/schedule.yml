name: Pegos Live Data Fetch & Predict

on:
  schedule:
    - cron: "45 20/4 * * *"   # Her 4 saatte bir (UTC)
  workflow_dispatch:

jobs:
  fetch_and_upload:
    runs-on: ubuntu-latest
    timeout-minutes: 50

    env:
      AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
      CT0: ${{ secrets.CT0 }}
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
      TWITTER_USER: ${{ secrets.TWITTER_USER }}
      TWITTER_PASS: ${{ secrets.TWITTER_PASS }}
      KEEP_LAST: ${{ secrets.KEEP_LAST }}

    steps:
      # === REPO VE ORTAM ===
      - name: üì¶ Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: üåê Install Chrome & dependencies
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: üì¶ Install Python dependencies
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install jupyter

      # === DEBUG VE TOKEN KONTROL√ú ===
      - name: üß© Debug environment & Hugging Face token
        run: |
          set -e
          echo "=== DEBUG ENVIRONMENT ==="
          echo "HF_DATASET_REPO: ${HF_DATASET_REPO}"
          echo "HF_TOKEN length: ${#HF_TOKEN}"
          echo "AUTH_TOKEN length: ${#AUTH_TOKEN}"
          echo "CT0 length: ${#CT0}"
          python - <<'PY'
          import os
          from huggingface_hub import HfApi
          token = os.getenv("HF_TOKEN")
          repo = os.getenv("HF_DATASET_REPO")
          api = HfApi(token=token)
          try:
              print("‚úÖ HF Token Valid:", bool(api.whoami()))
              print("‚úÖ Repo Access:", bool(api.repo_info(repo_id=repo, repo_type='dataset')))
          except Exception as e:
              print("‚ö†Ô∏è HF Access Error:", e)
          PY
          echo "==========================="

      # === SCRAPER ===
      - name: üï∏Ô∏è Run scraper notebook (data.ipynb)
        env:
          AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
          CT0: ${{ secrets.CT0 }}
        run: |
          set -e
          echo "üöÄ Starting Pegos Twitter Scraper..."
          jupyter nbconvert --to notebook --execute data.ipynb --output output.ipynb --ExecutePreprocessor.timeout=1800
          echo "‚úÖ Scraper notebook executed successfully"

      # === UPLOAD ===
      - name: ‚òÅÔ∏è Upload scraped data to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          CSV="/tmp/data/$(date -u +%F)/pegos_output.csv"
          ALT="/tmp/data/$(date -u +%F)/latest.csv"
          if [ -f "${CSV}" ]; then
            echo "üì§ Uploading scraped CSV -> Hugging Face"
            export LOCAL_CSV="${CSV}"
            python upload_to_hf.py
          elif [ -f "${ALT}" ]; then
            echo "üì§ Uploading alternate CSV -> Hugging Face"
            export LOCAL_CSV="${ALT}"
            python upload_to_hf.py
          else
            echo "‚ö†Ô∏è No CSV found in /tmp/data ‚Äî skipping upload."
          fi

      # === BUILD DATASET ===
      - name: üß± Merge new tweets with BTC data
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "üîÑ Building merged Pegos dataset..."
          python build_dataset_hf.py || echo "‚ö†Ô∏è build_dataset_hf.py failed ‚Äî continuing"
          echo "‚úÖ Merged dataset uploaded."

      # üÜï DAILY RAW CHECK
      - name: üßæ Verify daily_raw.csv created
        run: |
          echo "üìÑ Checking daily_raw.csv existence:"
          ls -lah /tmp/data/daily_raw.csv || echo "‚ö†Ô∏è daily_raw.csv not found"

      # === CLEAN DATASET (DAILY RAW) ===
      - name: üßπ Clean latest dataset (remove zero-engagement)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "üßΩ Cleaning latest daily dataset..."
          python clean_dataset_hf.py || echo "‚ö†Ô∏è clean_dataset_hf.py failed ‚Äî continuing"
          echo "‚úÖ Cleaned dataset uploaded."

      # === PREDICT (LATEST ONLY) ===
      - name: ü§ñ Run Pegos Hybrid Prediction (cleaned only)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          set -e
          echo "üß† Running Pegos prediction (cleaned.csv only)..."
          python predict_with_model.py || echo "‚ö†Ô∏è predict_with_model.py failed ‚Äî continuing"
          echo "‚úÖ Prediction complete."

      # === VALIDASYON ===
      - name: üìú List HF dataset files
        if: always()
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
        run: |
          python - <<'PY'
          import os
          from huggingface_hub import HfApi
          api = HfApi(token=os.getenv("HF_TOKEN"))
          repo = os.getenv("HF_DATASET_REPO")
          print("üìÇ Hugging Face Repo ƒ∞√ßeriƒüi:")
          for f in api.list_repo_files(repo_id=repo, repo_type="dataset"):
              print(" -", f)
          PY

      # === DEBUG & RAPOR ===
      - name: üóÇÔ∏è Show workspace & temp data
        if: always()
        run: |
          echo "üìÅ Workspace contents:"
          ls -lah
          echo
          echo "üì¶ /tmp/data i√ßeriƒüi:"
          ls -lah /tmp/data || true
          echo
          echo "üì¶ /tmp/data alt klas√∂rleri:"
          find /tmp/data -type f || true

      - name: ü™∂ Show notebook summary
        if: always()
        run: |
          echo "üìú Notebook Output (summary):"
          head -n 200 output.ipynb | sed 's/\\n/\n/g' | grep -E "‚úÖ|‚ö†Ô∏è|‚ùå|tweet|Upload|Total" || true
