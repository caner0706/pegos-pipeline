#!/usr/bin/env python
# coding: utf-8

"""
Pegos Data Pipeline Notebook
--------------------------------
Bu dosya Hugging Face ve GitHub Actions ile otomatik olarak çalıştırılır.
Her çalışmada:
1. Güncel veriyi çeker
2. Veriyi işler / temizler
3. data/output.csv olarak kaydeder
"""

# ===============================
# 1️⃣ GEREKLİ KÜTÜPHANELER
# ===============================
import os
import pandas as pd
import time
from datetime import datetime

print("✅ Başlangıç zamanı:", datetime.now())

# ===============================
# 2️⃣ VERİ ÇEKME KISMI
# ===============================
# ⚠️ BURAYA KENDİ TWITTER / API / WEB SCRAPING KODUNU EKLEYECEKSİN ⚠️
# Örnek bir simülasyon olarak aşağıdaki kod veri çekiyormuş gibi davranıyor:
# Sen kendi kodunu bu bloğun yerine koyabilirsin.

### BURADA SENİN VERİ ÇEKME KODUN GELECEK ###
# Örneğin:
# from selenium import webdriver
# ...
# tweets = [...]  # veri listesi
# df = pd.DataFrame(tweets)

# Şimdilik test amaçlı sahte veri oluşturalım:
data = {
    "datetime": [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
    "text": ["Bitcoin price analysis and crypto trends"],
    "views": [12345],
    "likes": [320],
    "retweets": [45]
}
df = pd.DataFrame(data)

# ===============================
# 3️⃣ VERİ KONTROLÜ VE TEMİZLİĞİ
# ===============================
print("📊 Veri önizleme:")
print(df.head())

# Boş değer kontrolü
print("Boş değer sayısı:")
print(df.isna().sum())

# ===============================
# 4️⃣ CSV OLARAK KAYDETME
# ===============================
os.makedirs("data", exist_ok=True)

# CSV dosya ismi sabit olacak (Actions bu dosyayı yükleyecek)
output_path = "data/output.csv"
df.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"✅ Veri '{output_path}' konumuna kaydedildi.")

# ===============================
# 5️⃣ GÜNLÜK VE KONTROL
# ===============================
print("✅ İşlem tamamlandı:", datetime.now())
time.sleep(2)  # stabil kapanış için kısa bekleme

