{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pegos Twitter Scraper ‚Üí Upload to Hugging Face (no local files)\n",
    "# ============================================================\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from huggingface_hub import HfApi\n",
    "import io, os, traceback\n",
    "\n",
    "print(\"‚ñ∂Ô∏è Starting scrape process at\", datetime.utcnow().isoformat(), \"UTC\")\n",
    "\n",
    "# ========== ENV =============\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "HF_DATASET_REPO = os.getenv(\"HF_DATASET_REPO\")\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
    "remote_latest = \"data/latest.csv\"\n",
    "remote_archive = f\"data/blockchain_tweets_{ts}.csv\"\n",
    "\n",
    "# ========== SELENIUM CONFIG ==========\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "url = \"https://x.com/i/flow/login\"\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Kullanƒ±cƒ± adƒ±\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH,\n",
    "        '/html/body/div/div/div/div[1]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[4]/label/div/div[2]/div/input')))\n",
    "    usernameInput = driver.find_element(By.XPATH,\n",
    "        '/html/body/div/div/div/div[1]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[4]/label/div/div[2]/div/input')\n",
    "    usernameInput.send_keys(\"canergiden007\")\n",
    "    time.sleep(1)\n",
    "    usernameInput.send_keys(Keys.ENTER)\n",
    "\n",
    "    # ≈ûifre\n",
    "    password_field_locator = (By.XPATH,\n",
    "        '/html/body/div/div/div/div[1]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div/div[3]/div/label/div/div[2]/div[1]/input')\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located(password_field_locator))\n",
    "    passwordInput = driver.find_element(*password_field_locator)\n",
    "    passwordInput.send_keys(\"Canergiden007@\")\n",
    "    passwordInput.send_keys(Keys.ENTER)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Anahtar kelimeler\n",
    "    keywords = [\"blockchain\",]\n",
    "    tweetArr = []\n",
    "\n",
    "    for stock in keywords:\n",
    "        url = \"https://x.com/search?q=\" + stock\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "        for _ in range(30):  # optimize: 100 yerine 30 scroll\n",
    "            driver.execute_script(\"window.scrollBy(0, 1200);\")\n",
    "            time.sleep(1.5)\n",
    "            htmlContent = driver.page_source\n",
    "            soap = BeautifulSoup(htmlContent, \"html.parser\")\n",
    "            tweetBodies = soap.find_all(\"article\")\n",
    "\n",
    "            for tweetBody in tweetBodies:\n",
    "                tweetObj = {}\n",
    "                tweet = tweetBody.find(attrs={\"data-testid\": \"tweetText\"})\n",
    "                if not tweet or not tweet.text:\n",
    "                    continue\n",
    "                tweetObj[\"tweet\"] = tweet.text\n",
    "                timeElement = tweetBody.find(\"time\")\n",
    "                tweetObj[\"time\"] = timeElement[\"datetime\"] if timeElement else \"-\"\n",
    "                interactionCountItems = tweetBody.find_all(attrs={\"data-testid\":\"app-text-transition-container\"})\n",
    "                for counter, interactionCount in enumerate(interactionCountItems):\n",
    "                    result = interactionCount.text or \"0\"\n",
    "                    if \"B\" in result:\n",
    "                        result = float(result.replace(\"B\", \"\").strip()) * 1000\n",
    "                    elif \"Mn\" in result:\n",
    "                        result = float(result.replace(\"Mn\", \"\").strip()) * 1_000_000\n",
    "                    try:\n",
    "                        result = int(float(result))\n",
    "                    except:\n",
    "                        result = 0\n",
    "                    if counter == 0:\n",
    "                        tweetObj[\"comment\"] = result\n",
    "                    elif counter == 1:\n",
    "                        tweetObj[\"retweet\"] = result\n",
    "                    elif counter == 2:\n",
    "                        tweetObj[\"like\"] = result\n",
    "                    elif counter == 3:\n",
    "                        tweetObj[\"see_count\"] = result\n",
    "                tweetObj[\"code\"] = stock\n",
    "                tweetArr.append(tweetObj)\n",
    "\n",
    "    df = pd.DataFrame(tweetArr)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"‚úÖ Scraped rows:\", len(df))\n",
    "\n",
    "    # ========== Upload to Hugging Face ==========\n",
    "    csv_buf = io.StringIO()\n",
    "    df.to_csv(csv_buf, index=False)\n",
    "    csv_bytes = csv_buf.getvalue().encode(\"utf-8\")\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=io.BytesIO(csv_bytes),\n",
    "        path_in_repo=remote_archive,\n",
    "        repo_id=HF_DATASET_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    print(\"üì§ Uploaded archive:\", remote_archive)\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=io.BytesIO(csv_bytes),\n",
    "        path_in_repo=remote_latest,\n",
    "        repo_id=HF_DATASET_REPO,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    print(\"‚úÖ Updated:\", remote_latest)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error during scraping:\", traceback.format_exc())\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"üõë Driver closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
