# =====================================================
# Pegos Prediction (Full Detail â€“ All Columns)
# =====================================================
import os
import joblib
import torch
import numpy as np
import pandas as pd
from datetime import datetime
from transformers import AutoTokenizer, AutoModel
from huggingface_hub import hf_hub_download, upload_file

# === Ortam deÄŸiÅŸkenleri ===
HF_TOKEN = os.getenv("HF_TOKEN")
HF_DATASET_REPO = os.getenv("HF_DATASET_REPO")

print("ğŸ¤– Pegos Prediction (tÃ¼m sÃ¼tunlu detaylÄ± Ã§Ä±ktÄ±) baÅŸlatÄ±ldÄ±")

# === 1ï¸âƒ£ Yeni batch verisini indir (latest.csv) ===
p = hf_hub_download(
    repo_id=HF_DATASET_REPO,
    filename="data/cleaned.csv",
    repo_type="dataset",
    token=HF_TOKEN,
)
df = pd.read_csv(p, encoding="utf-8-sig")
print(f"âœ… Yeni batch veri yÃ¼klendi ({len(df)} satÄ±r)")

if df.empty:
    print("âš ï¸ Yeni veri yok, Ã§Ä±kÄ±lÄ±yor.")
    exit()

# === 2ï¸âƒ£ Model dosyalarÄ± ===
clf = joblib.load("pegos_lightgbm.pkl")
reg = joblib.load("pegos_regressor.pkl")
scaler = joblib.load("scaler.pkl")

# === 3ï¸âƒ£ BERT modeli ===
tok = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")
bert = AutoModel.from_pretrained("dbmdz/bert-base-turkish-cased")
device = "cuda" if torch.cuda.is_available() else "cpu"
bert.to(device).eval()

# === 4ï¸âƒ£ SayÄ±sal kolonlarÄ± dÃ¼zenle ===
for c in ["comment", "retweet", "like", "see_count"]:
    if c not in df.columns:
        df[c] = 0
df[["comment", "retweet", "like", "see_count"]] = df[["comment", "retweet", "like", "see_count"]].fillna(0)
X_num = scaler.transform(df[["comment", "retweet", "like", "see_count"]])

# === 5ï¸âƒ£ Metin embedding iÅŸlemi ===
def embed(texts, bs=16):
    embs = []
    with torch.no_grad():
        for i in range(0, len(texts), bs):
            batch = [str(t) for t in texts[i:i+bs]]
            tks = tok(batch, padding=True, truncation=True, max_length=128, return_tensors="pt").to(device)
            out = bert(**tks).last_hidden_state[:, 0, :].cpu().numpy()
            embs.append(out)
    return np.vstack(embs)

X_text = embed(df["tweet"].tolist())
X = np.hstack([X_text, X_num])

# === 6ï¸âƒ£ Model tahminleri ===
df["pred_label"] = clf.predict(X)                     # 1 = yÃ¼kseliÅŸ, 0 = dÃ¼ÅŸÃ¼ÅŸ
df["pred_proba"] = clf.predict_proba(X)[:, 1]         # gÃ¼ven olasÄ±lÄ±ÄŸÄ±
df["pred_diff"] = reg.predict(X)                      # fiyat farkÄ± (oransal)
df["AI_Model_Tahmini (%)"] = (df["pred_diff"] * 100).round(2)

# === 7ï¸âƒ£ YÃ¶n ve gÃ¼ven sÃ¼tunlarÄ± ===
df["AI_Model_Yonu"] = np.where(
    df["pred_diff"] > 0,
    "ğŸ“ˆ ArtÄ±ÅŸ Bekleniyor",
    np.where(df["pred_diff"] < 0, "ğŸ“‰ DÃ¼ÅŸÃ¼ÅŸ Bekleniyor", "âš–ï¸ DeÄŸiÅŸim Yok")
)
df["Tahmin"] = df["pred_label"].map({1: "ğŸ“ˆ YÃœKSELÄ°Å", 0: "ğŸ“‰ DÃœÅÃœÅ"})
df["GÃ¼ven (%)"] = (df["pred_proba"] * 100).round(1)

# === 8ï¸âƒ£ Ä°ÅŸlem gÃ¼nÃ¼ ===
df["prediction_day"] = datetime.utcnow().strftime("%Y-%m-%d")

# === 9ï¸âƒ£ SÃ¼tun sÄ±ralamasÄ±nÄ± dÃ¼zenle ===
ordered_cols = [
    "tweet", "comment", "retweet", "like", "see_count",
    "pred_label", "pred_proba", "pred_diff",
    "AI_Model_Tahmini (%)", "AI_Model_Yonu",
    "Tahmin", "GÃ¼ven (%)", "prediction_day"
]
df = df[ordered_cols]

# === ğŸ”Ÿ Kaydet & YÃ¼kle ===
os.makedirs("/tmp/data", exist_ok=True)
out_path = "/tmp/data/predict.csv"
df.to_csv(out_path, index=False, encoding="utf-8-sig")

upload_file(
    path_or_fileobj=out_path,
    path_in_repo="data/predict.csv",
    repo_id=HF_DATASET_REPO,
    repo_type="dataset",
    token=HF_TOKEN,
)

print(f"ğŸš€ predict.csv Hugging Faceâ€™e yÃ¼klendi ({len(df)} satÄ±r)")
print(f"ğŸ“… prediction_day: {df['prediction_day'].iloc[0]}")
