# =====================================================
# Pegos Daily Dataset Cleaning Script ğŸ§¹
# =====================================================
import os
import pandas as pd
from huggingface_hub import HfApi, hf_hub_download

# ------------------------------------------
# Ortam deÄŸiÅŸkenleri
# ------------------------------------------
HF_TOKEN = os.getenv("HF_TOKEN")
HF_DATASET_REPO = os.getenv("HF_DATASET_REPO")
if not HF_TOKEN or not HF_DATASET_REPO:
    raise RuntimeError("âŒ HF_TOKEN veya HF_DATASET_REPO tanÄ±mlÄ± deÄŸil!")

api = HfApi(token=HF_TOKEN)

# ------------------------------------------
# En gÃ¼ncel klasÃ¶rÃ¼ bul
# ------------------------------------------
print("ğŸ“‚ GÃ¼nlÃ¼k klasÃ¶rler listeleniyor...")
files = api.list_repo_files(repo_id=HF_DATASET_REPO, repo_type="dataset")
daily_folders = sorted(
    list({f.split("/")[1] for f in files if f.startswith("data/") and len(f.split("/")) > 2})
)
if not daily_folders:
    raise RuntimeError("âŒ GÃ¼nlÃ¼k klasÃ¶r bulunamadÄ±!")
latest_day = daily_folders[-1]
print(f"ğŸ“… En gÃ¼ncel klasÃ¶r: {latest_day}")

merged_file = f"data/{latest_day}/merged.csv"
print(f"ğŸ“¥ Dosya indiriliyor: {merged_file}")

path = hf_hub_download(
    repo_id=HF_DATASET_REPO,
    filename=merged_file,
    repo_type="dataset",
    token=HF_TOKEN,
)

df = pd.read_csv(path)
print(f"âœ… Veri yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")

# ------------------------------------------
# 1ï¸âƒ£ Kolon seÃ§imi
# ------------------------------------------
columns_needed = [
    "tweet",
    "comment",
    "retweet",
    "like",
    "see_count",
    "time",
    "open",
    "close",
    "diff"
]
df = df[[c for c in columns_needed if c in df.columns]].copy()
df.rename(columns={
    "open": "AÃ§Ä±lÄ±ÅŸ FiyatÄ± (USD)",
    "close": "KapanÄ±ÅŸ FiyatÄ± (USD)",
    "diff": "Fark (USD)"
}, inplace=True)

# ------------------------------------------
# 2ï¸âƒ£ Yinelenen ve eksik veriler
# ------------------------------------------
before = len(df)
df.drop_duplicates(subset=["tweet", "time"], inplace=True)
print(f"ğŸ§© Yinelenenler: {before - len(df)} satÄ±r silindi.")

before = len(df)
df.dropna(subset=["tweet", "AÃ§Ä±lÄ±ÅŸ FiyatÄ± (USD)", "KapanÄ±ÅŸ FiyatÄ± (USD)"], inplace=True)
print(f"ğŸš¿ Eksik deÄŸer temizliÄŸi: {before - len(df)} satÄ±r silindi.")

# ------------------------------------------
# 3ï¸âƒ£ AykÄ±rÄ± deÄŸer analizi (soft filter)
# ------------------------------------------
def soft_outlier_filter(series):
    q1 = series.quantile(0.01)
    q3 = series.quantile(0.99)
    return series.between(q1, q3)

numeric_cols = ["comment", "retweet", "like", "see_count", "Fark (USD)"]
for col in numeric_cols:
    if col in df.columns:
        mask = soft_outlier_filter(df[col])
        removed = (~mask).sum()
        df = df[mask]
        print(f"âš–ï¸ {col}: {removed} uÃ§ deÄŸer kaldÄ±rÄ±ldÄ±.")

# ------------------------------------------
# 4ï¸âƒ£ Tarih kontrolÃ¼
# ------------------------------------------
df["time"] = pd.to_datetime(df["time"], errors="coerce")
df = df.dropna(subset=["time"])
df = df[df["time"] > "2020-01-01"]

# ------------------------------------------
# 5ï¸âƒ£ Kaydet ve HF'ye yÃ¼kle
# ------------------------------------------
output_path = f"/tmp/cleaned_{latest_day}.csv"
df.to_csv(output_path, index=False)
print(f"ğŸ’¾ TemizlenmiÅŸ veri kaydedildi: {output_path} ({len(df)} satÄ±r)")

remote_path = f"data/{latest_day}/cleaned.csv"
api.upload_file(
    path_or_fileobj=output_path,
    path_in_repo=remote_path,
    repo_id=HF_DATASET_REPO,
    repo_type="dataset",
)
print(f"ğŸš€ Hugging Face'e yÃ¼kleme tamamlandÄ±: {remote_path}")