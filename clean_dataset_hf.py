# =====================================================
# Pegos Dataset Cleaning (Safe & Minimal Loss Version)
# =====================================================
import os
import pandas as pd
from datetime import datetime
from huggingface_hub import hf_hub_download, upload_file

# === Ortam deÄŸiÅŸkenleri ===
HF_TOKEN = os.getenv("HF_TOKEN")
HF_DATASET_REPO = os.getenv("HF_DATASET_REPO")
TODAY = datetime.utcnow().strftime("%Y-%m-%d")

# === Veri indirme ===
target_file = f"data/{TODAY}/pegos_final_dataset.csv"
print(f"ğŸ“¥ Ä°ndiriliyor: {target_file}")

path = hf_hub_download(
    repo_id=HF_DATASET_REPO,
    filename=target_file,
    repo_type="dataset",
    token=HF_TOKEN,
)
df = pd.read_csv(path, encoding="utf-8")
print(f"âœ… Veri yÃ¼klendi ({len(df)} satÄ±r)")

# === 1ï¸âƒ£ Temel temizlik ===
df.drop_duplicates(subset=["tweet", "time"], inplace=True)
df.dropna(subset=["tweet"], inplace=True)
if {"open", "close"}.issubset(df.columns):
    df.dropna(subset=["open", "close"], inplace=True)

# === 2ï¸âƒ£ EtkileÅŸim filtresi (hepsi 0 olanlarÄ± sil) ===
if all(col in df.columns for col in ["comment", "retweet", "like", "see_count"]):
    before = len(df)
    df = df[~((df["comment"] == 0) & (df["retweet"] == 0) &
              (df["like"] == 0) & (df["see_count"] == 0))]
    removed = before - len(df)
    print(f"ğŸ§¹ SÄ±fÄ±r etkileÅŸimli {removed} satÄ±r temizlendi.")

# === 3ï¸âƒ£ Tarih ve zaman doÄŸrulama ===
df["time"] = pd.to_datetime(df["time"], errors="coerce")
df = df.dropna(subset=["time"])
df = df[df["time"] >= "2020-01-01"]

# === 4ï¸âƒ£ Outlier temizliÄŸi (devre dÄ±ÅŸÄ±) ===
# (Veri kaybÄ±nÄ± Ã¶nlemek iÃ§in bu adÄ±m atlandÄ±)
# for col in ["comment", "retweet", "like", "see_count", "diff"]:
#     if col in df.columns and len(df) > 0:
#         q1, q3 = df[col].quantile(0.01), df[col].quantile(0.99)
#         df = df[df[col].between(q1, q3)]

# === 5ï¸âƒ£ Kaydet ve yÃ¼kle ===
os.makedirs(f"/tmp/{TODAY}", exist_ok=True)
out_path = f"/tmp/{TODAY}/cleaned.csv"
df.to_csv(out_path, index=False, encoding="utf-8")
print(f"ğŸ’¾ Kaydedildi ({len(df)} satÄ±r)")

upload_file(
    path_or_fileobj=out_path,
    path_in_repo=f"data/{TODAY}/cleaned.csv",
    repo_id=HF_DATASET_REPO,
    repo_type="dataset",
    token=HF_TOKEN,
)
print("ğŸš€ TemizlenmiÅŸ veri Hugging Faceâ€™e yÃ¼klendi.")
